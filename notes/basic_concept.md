
# NOTES

## epochs & batch size

Gradient Descent
learning rate
cost/lost/loss
dataset
epoch/batch size/iteration
weights/step

dataset → epoch → batch → iteration

under-fitting/optimal/overfitting
accuracy/predication
bias/variance
bias = simple/simplistic assumption/poor performance
variance = sensitivity/fluctuations
high bias = under fitting
low bias / low variance = good fitting
high variance = over fitting
training data/testing data
training error/test error
under fitting = high training error/training error close to test error/high bias

1. [[Epoch vs Batch Size vs Iterations](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)]
2. [[ML | Under fitting and Overfitting](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning)]
3. [[Under fitting Vs Just right Vs Overfitting in Machine learning](https://www.kaggle.com/discussions/getting-started/166897)]

## errors

prediction error
bias error = |predication value - actual value|
           = error due to squared bias
variance error = error due to variance

1. [[3 Types of Errors in Machine Learning that You Should Know!](https://multimatics.co.id/insight/jan/3-types-of-errors-in-machine-learning-that-you-should-know)]
2. [[Ask a Data Scientist: The Bias vs. Variance Tradeoff](https://insideainews.com/2014/10/22/ask-data-scientist-bias-vs-variance-tradeoff/)]
3. [[What Is the Difference Between Bias and Variance?](https://www.mastersindatascience.org/learning/difference-between-bias-and-variance/)]

## ML 101

Goal

Step 1 : study article [[Machine Learning 101] (https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a)]
Step 2 : modify code so that it can be run under *WANDB*.

1. 
